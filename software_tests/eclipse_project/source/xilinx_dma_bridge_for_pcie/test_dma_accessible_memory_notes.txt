Note: The test_dma_accessible_memory program these results were using has been replaced with test_dma_bridge.

Differences between the programs are, which impact on the reported results are:
a. test_dma_accessible_memory only supported Memory Mapped endpoints.
   Whereas test_dma_bridge also supports stream AXI4 endpoints.
b. The test_dma_accessible_memory options for controlling the address alignment for transfers haven't
   been added to test_dma_bridge, as with test_dma_accessible_memory changing the address alignment didn't result in
   any measureable change in transfer performance.
c. test_dma_accessible_memory only transfers and times in one H2C to C2H direction at a time.
   Whereas test_dma_bridge attempts to overlap the H2C and C2H transfers with multiple descriptors used for each transfer.
   As a result, the transfer rate reported by test_dma_bridge is the simultaneous rate in each direction.
d. test_dma_bridge reports the time taken to generate and verify the test pattern in host memory, in addition to the
   transfer time.

1. Tests using TOSING_160T_dma_ddr3 design with no IOMMU
========================================================

The TOSING_160T_dma_ddr3 design was used in a i5-2310 system which doesn't have an IOMMU.

The following was temporarily added to the command line to allocate physical memory for the cmem_gdb_access driver:
  memmap=2050M$4G,1M$6146M

Using default arguments, which results in transfers which are not word aligned:
[mr_halfword@haswell-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f2c2a43a000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f2be9a24000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f2ba9a24000 
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 836.020419 (Mbytes/sec)
  Mean = 836.178502 (Mbytes/sec)
   Max = 836.356822 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.552247 (Mbytes/sec)
  Mean = 834.702048 (Mbytes/sec)
   Max = 834.819760 (Mbytes/sec)
Memory Driver closed 

With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@haswell-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f0b4e281000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f0b0d86b000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f0acd86b000 
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 835.930008 (Mbytes/sec)
  Mean = 836.092384 (Mbytes/sec)
   Max = 836.261624 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.649540 (Mbytes/sec)
  Mean = 834.695353 (Mbytes/sec)
   Max = 834.785671 (Mbytes/sec)
Memory Driver closed 


2. Test using Nitefury Project-0 and dma_blkram designs with IOMMU enabled
==========================================================================

The Nitefury Project-0 and dma_blkram designs were used in a W-2123 system with the IOMMU and secure boot enabled. 

Using default arguments, which for the Nitefury Project-0 design results in transfers which are not word aligned:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 87
Enabling bus master for 0000:2e:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1446.425221 (Mbytes/sec)
  Mean = 1755.580095 (Mbytes/sec)
   Max = 1757.884902 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1375.132746 (Mbytes/sec)
  Mean = 1405.979819 (Mbytes/sec)
   Max = 1406.244636 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:2e:00.0 IOMMU group 87
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.816988 (Mbytes/sec)
  Mean = 1325.819613 (Mbytes/sec)
   Max = 1325.820986 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.562019 (Mbytes/sec)
  Mean = 1130.570527 (Mbytes/sec)
   Max = 1130.578031 (Mbytes/sec)


With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 87
Enabling bus master for 0000:2e:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1706.175450 (Mbytes/sec)
  Mean = 1754.846234 (Mbytes/sec)
   Max = 1756.536863 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1361.628255 (Mbytes/sec)
  Mean = 1405.971441 (Mbytes/sec)
   Max = 1406.229548 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:2e:00.0 IOMMU group 87
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.805850 (Mbytes/sec)
  Mean = 1325.818622 (Mbytes/sec)
   Max = 1325.820957 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.579250 (Mbytes/sec)
  Mean = 1130.612202 (Mbytes/sec)
   Max = 1130.638191 (Mbytes/sec)


3. Tests using TOSING_160T_dma_ddr3 design with broken timing and with no IOMMU
===============================================================================

The tests in 1) used the TOSING_160T_dma_ddr3 design from commit 2c8c960, and the resulting bandwidth was lower
than with the Nitefury Project-0 even though:
a. TOSING_160T_dma_ddr3 should have DDR3-1066 with a 32-bit bus.
b. Nitefury Project-0 has DDR3-800 with a 16-bit bus.

I.e. lower bandwidth for TOSING_160T_dma_ddr3 even though in theory the DDR3 interface had more bandwidth.

Tried changing the PHYRatio from 4:1 to 2:1, which also involved increasing the TimePeriod from 1500 to 1875
in order allow the PHYRatio to be changed.

Differences were:
$ git diff -U0 | cat
diff --git a/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl b/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
index 655bda2..471322f 100644
--- a/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
+++ b/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
@@ -6 +6 @@
-# Generated by Vivado on Sat Sep 09 19:56:21 BST 2023
+# Generated by Vivado on Sun Oct 01 22:54:57 BST 2023
@@ -172,0 +173 @@ set_property -name "top" -value "TOSING_160T_dma_ddr3_wrapper" -objects $obj
+set_property -name "top_auto_set" -value "0" -objects $obj
@@ -221,0 +223 @@ set_property -name "top" -value "TOSING_160T_dma_ddr3_wrapper" -objects $obj
+set_property -name "top_auto_set" -value "0" -objects $obj
@@ -318 +320 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <TimePeriod>1500</TimePeriod>}
+   puts $mig_prj_file {    <TimePeriod>1875</TimePeriod>}
@@ -320 +322 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <PHYRatio>4:1</PHYRatio>}
+   puts $mig_prj_file {    <PHYRatio>2:1</PHYRatio>}
@@ -323 +325 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <MMCM_VCO>666</MMCM_VCO>}
+   puts $mig_prj_file {    <MMCM_VCO>1066</MMCM_VCO>}
@@ -425 +427 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <mrCasLatency name="CAS Latency">9</mrCasLatency>}
+   puts $mig_prj_file {    <mrCasLatency name="CAS Latency">7</mrCasLatency>}
@@ -439 +441 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <mr2CasWriteLatency name="CAS write latency">7</mr2CasWriteLatency>}
+   puts $mig_prj_file {    <mr2CasWriteLatency name="CAS write latency">6</mr2CasWriteLatency>}
@@ -531 +533 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-  set str_mig_file_name mig_b.prj
+  set str_mig_file_name mig_a.prj
@@ -539 +541 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-    CONFIG.XML_INPUT_FILE {mig_b.prj} \
+    CONFIG.XML_INPUT_FILE {mig_a.prj} \

However, with this change then Vivado reported 806 failing timing endpoints for Intra-Clock Paths for clk_pll_i
Worst case slack was -1.8 ns for setup.

Trying this FPGA image did report data validation errors, and didn't change the bandwidth achieved by the DMA transfers:
$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f64ec05c000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f64ab646000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f646b646000 
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
DDR word[274] actual=0xaa541f0a expected=0xaa541f02
DDR word[1586] actual=0x476aedaa expected=0x476aeda2
DDR word[594] actual=0x20bf74a expected=0x20bf742
DDR word[2381] actual=0x2b742e8f expected=0x2b74224b
DDR word[1186] actual=0x47d5195a expected=0x47d51952
DDR word[461] actual=0x3e93a70f expected=0x3e93a2cb
DDR word[460] actual=0x31081e1c expected=0x39081e1c
DDR word[209] actual=0xb57114af expected=0xb571142f
DDR word[2125] actual=0x85279b8f expected=0x8527114b
DDR word[562] actual=0xa7b039aa expected=0xa7b039a2
DDR word[50] actual=0xa48adfaa expected=0xa48adfa2
DDR word[737] actual=0xc1f752bf expected=0xc1f7523f
DDR word[2545] actual=0x31ae79cf expected=0x31ae794f
DDR word[844] actual=0x1ce5bd9c expected=0x18e5bd9c
DDR word[205] actual=0x1664140f expected=0x166491cb
DDR word[322] actual=0x4b23f17a expected=0x4b23f172
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.598262 (Mbytes/sec)
  Mean = 835.675384 (Mbytes/sec)
   Max = 836.468913 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 833.912205 (Mbytes/sec)
  Mean = 834.458438 (Mbytes/sec)
   Max = 834.801162 (Mbytes/sec)
Memory Driver closed 

Therefore, didn't commit the change to the DDR3 MIG configuration in the TOSING_160T_dma_ddr3 design.

However, this did show the use of the test pattern in test_dma_accessible_memory detecting errors.

As for the achieved DMA bandwidth with TOSING_160T_dma_ddr3 being lower than Nitefury Project-0,
the different FPGA images were tested in different PC and so the PCIe Root Bridge / memory in the
two PCs might be impacting the achieved bandwidth.


4. Test using Nitefury Project-0 and TEF1001_dma_ddr3 designs with IOMMU enabled; issues with host memory sizes
===============================================================================================================

The Nitefury Project-0 and TEF1001_dma_ddr3 designs were used in a W-2123 system with the IOMMU and secure boot enabled. 

This did show some issues with host memory sizes.

The TEF1001_dma_ddr3 has 8G of DDR3 memory, and the PC only has 16G of memory.
Therefore, with default command line arguments in which a host buffer of the entire DDR3 memory is attempted
to be allocated for both host-to-card and card-to-host transfers expected the test for TEF1001_dma_ddr3
to fail to allocate memory, although both 8G allocations failed:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
VFIO_IOMMU_MAP_DMA of size 8589934592 failed : Operation not permitted
VFIO_IOMMU_MAP_DMA of size 8589934592 failed : Operation not permitted
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.814544 (Mbytes/sec)
  Mean = 1325.823781 (Mbytes/sec)
   Max = 1325.825198 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.581147 (Mbytes/sec)
  Mean = 1130.601240 (Mbytes/sec)
   Max = 1130.625491 (Mbytes/sec)


In use a 1G size for host-to-card and card-to-host transfers the TEF1001_dma_ddr3 test passes,
but the Nitefury Project-0 then fails during VFIO_IOMMU_MAP_DMA on attempting to allocate the 2nd host buffer:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -l 1073741824 -m 1073741824
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1771.180385 (Mbytes/sec)
  Mean = 1771.198526 (Mbytes/sec)
   Max = 1771.204185 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1291.505285 (Mbytes/sec)
  Mean = 1291.506313 (Mbytes/sec)
   Max = 1291.507205 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
VFIO_IOMMU_MAP_DMA of size 1073741824 failed : Operation not permitted


With a 512M size for host-to-card transfers and a 1G size for card-to-host transfers the tests on both FPGAs pass:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -l 1073741824 -m 536870912
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0x2
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 32 transfers of 536870912 bytes:
   Min = 1771.146331 (Mbytes/sec)
  Mean = 1771.212862 (Mbytes/sec)
   Max = 1771.252126 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1291.505793 (Mbytes/sec)
  Mean = 1291.507043 (Mbytes/sec)
   Max = 1291.507871 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0x2
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 32 transfers of 536870912 bytes:
   Min = 1325.773629 (Mbytes/sec)
  Mean = 1325.811476 (Mbytes/sec)
   Max = 1325.816964 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.579775 (Mbytes/sec)
  Mean = 1130.596196 (Mbytes/sec)
   Max = 1130.610757 (Mbytes/sec)


5. Test using Nitefury Project-0 and TEF1001_dma_ddr3 designs with IOMMU enabled; with simple work-around to IOVA allocation
============================================================================================================================

The test_dma_accessible_memory program was modified to set the IOVA to start at 4G, as a simple way to avoid
the reserved IOVA addresses in the lower 4G addresses (may not work on a different computer).

With no arguments, so attempt to allocate all 16G of host memory for card-to-host and host-to-card transfers
for the TEF1001_dma_ddr3 design the desktop session became unresponsive and after a number of minutes the
program was terminated by the OOM killer, which is not unexpected given the program is trying to lock
all physical memory for VFIO DMA:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
Killed


Using -l to set c2h_transfer_size as 4G while leaving the h2c_transfer_size as 8G still got killed.


The combinations of 1G for one direction and the full 8G for the other direction allowed the test to pass:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -l 1073741824
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0xfffffff [5]=0xfffffff [6]=0xfffffff [7]=0xfffffff [8]=0xfffffff [9]=0xfffffff [10]=0xfffffff [11]=0xfffffff [12]=0xfffffff [13]=0xfffffff [14]=0xfffffff [15]=0xfffffff [16]=0xfffffff [17]=0xfffffff [18]=0xfffffff [19]=0xfffffff [20]=0xfffffff [21]=0xfffffff [22]=0xfffffff [23]=0xfffffff [24]=0xfffffff [25]=0xfffffff [26]=0xfffffff [27]=0xfffffff [28]=0xfffffff [29]=0xfffffff [30]=0xfffffff [31]=0xfffffff [32]=0x20
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1771.280438 (Mbytes/sec)
  Mean = 1771.360846 (Mbytes/sec)
   Max = 1771.441261 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1291.492831 (Mbytes/sec)
  Mean = 1291.497032 (Mbytes/sec)
   Max = 1291.505779 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.803807 (Mbytes/sec)
  Mean = 1325.811377 (Mbytes/sec)
   Max = 1325.812963 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.449948 (Mbytes/sec)
  Mean = 1130.591393 (Mbytes/sec)
   Max = 1130.624666 (Mbytes/sec)

[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -m 1073741824
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0xfffffff [5]=0xfffffff [6]=0xfffffff [7]=0xfffffff [8]=0xfffffff [9]=0xfffffff [10]=0xfffffff [11]=0xfffffff [12]=0xfffffff [13]=0xfffffff [14]=0xfffffff [15]=0xfffffff [16]=0xfffffff [17]=0xfffffff [18]=0xfffffff [19]=0xfffffff [20]=0xfffffff [21]=0xfffffff [22]=0xfffffff [23]=0xfffffff [24]=0xfffffff [25]=0xfffffff [26]=0xfffffff [27]=0xfffffff [28]=0xfffffff [29]=0xfffffff [30]=0xfffffff [31]=0xfffffff [32]=0x20
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1771.268541 (Mbytes/sec)
  Mean = 1771.476983 (Mbytes/sec)
   Max = 1771.701331 (Mbytes/sec)
card-to-host DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1291.513784 (Mbytes/sec)
  Mean = 1291.513912 (Mbytes/sec)
   Max = 1291.514039 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.819395 (Mbytes/sec)
  Mean = 1325.824069 (Mbytes/sec)
   Max = 1325.825873 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.521574 (Mbytes/sec)
  Mean = 1130.583159 (Mbytes/sec)
   Max = 1130.599609 (Mbytes/sec)


6. Test using NiteFury_dma_ddr3 and Nitefury Project-0 and dma_blkram designs with IOMMU enabled
================================================================================================

The NiteFury_dma_ddr3 and dma_blkram designs were used in a W-2123 system with the IOMMU and secure boot enabled. 

Using default arguments, which for the NiteFury_dma_ddr3 design results in transfers which are not word aligned:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7024) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1691.084323 (Mbytes/sec)
  Mean = 1754.170372 (Mbytes/sec)
   Max = 1756.746131 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1376.127332 (Mbytes/sec)
  Mean = 1405.894362 (Mbytes/sec)
   Max = 1406.117243 (Mbytes/sec)
Testing NiteFury_dma_ddr3 design with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1332.395773 (Mbytes/sec)
  Mean = 1332.618110 (Mbytes/sec)
   Max = 1332.857297 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1150.290232 (Mbytes/sec)
  Mean = 1150.305574 (Mbytes/sec)
   Max = 1150.319972 (Mbytes/sec)


With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7024) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1692.506130 (Mbytes/sec)
  Mean = 1754.368640 (Mbytes/sec)
   Max = 1757.596797 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1376.596249 (Mbytes/sec)
  Mean = 1405.872580 (Mbytes/sec)
   Max = 1406.122271 (Mbytes/sec)
Testing NiteFury_dma_ddr3 design with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1332.364157 (Mbytes/sec)
  Mean = 1332.535455 (Mbytes/sec)
   Max = 1332.767980 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1150.297237 (Mbytes/sec)
  Mean = 1150.305676 (Mbytes/sec)
   Max = 1150.316809 (Mbytes/sec)

Also, the achieved bandwidth with the NiteFury_dma_ddr3 design (created from scratch) is the same as the
pre-flashed Nitefury Project-0 example design from the manufacturer.


7. Test using Nitefury Project-0 and TEF1001_dma_ddr3 designs with IOMMU enabled; with memory doubled to 32 GiB
===============================================================================================================

Since the previous test 5, the memory in the PC has been doubled from 16 GiB to 32 GiB.
This means that now allocate 16 GiB for the TEF1001_dma_ddr3 design (8 GiB for a buffer in each direction).

With no arguments the test passes on both designs:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7024) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0xfffffff [5]=0xfffffff [6]=0xfffffff [7]=0xfffffff [8]=0xfffffff [9]=0xfffffff [10]=0xfffffff [11]=0xfffffff [12]=0xfffffff [13]=0xfffffff [14]=0xfffffff [15]=0xfffffff [16]=0xfffffff [17]=0xfffffff [18]=0xfffffff [19]=0xfffffff [20]=0xfffffff [21]=0xfffffff [22]=0xfffffff [23]=0xfffffff [24]=0xfffffff [25]=0xfffffff [26]=0xfffffff [27]=0xfffffff [28]=0xfffffff [29]=0xfffffff [30]=0xfffffff [31]=0xfffffff [32]=0x20
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0xfffffff [5]=0xfffffff [6]=0xfffffff [7]=0xfffffff [8]=0xfffffff [9]=0xfffffff [10]=0xfffffff [11]=0xfffffff [12]=0xfffffff [13]=0xfffffff [14]=0xfffffff [15]=0xfffffff [16]=0xfffffff [17]=0xfffffff [18]=0xfffffff [19]=0xfffffff [20]=0xfffffff [21]=0xfffffff [22]=0xfffffff [23]=0xfffffff [24]=0xfffffff [25]=0xfffffff [26]=0xfffffff [27]=0xfffffff [28]=0xfffffff [29]=0xfffffff [30]=0xfffffff [31]=0xfffffff [32]=0x20
Test pattern pass
host-to-card DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1771.545187 (Mbytes/sec)
  Mean = 1771.557939 (Mbytes/sec)
   Max = 1771.570692 (Mbytes/sec)
card-to-host DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1291.517098 (Mbytes/sec)
  Mean = 1291.517648 (Mbytes/sec)
   Max = 1291.518197 (Mbytes/sec)
Testing NiteFury_dma_ddr3 design with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1332.354353 (Mbytes/sec)
  Mean = 1332.612714 (Mbytes/sec)
   Max = 1332.853373 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1150.285277 (Mbytes/sec)
  Mean = 1150.300955 (Mbytes/sec)
   Max = 1150.308007 (Mbytes/sec)


With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7024) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0xffff000 [5]=0xffff000 [6]=0xffff000 [7]=0xffff000 [8]=0xffff000 [9]=0xffff000 [10]=0xffff000 [11]=0xffff000 [12]=0xffff000 [13]=0xffff000 [14]=0xffff000 [15]=0xffff000 [16]=0xffff000 [17]=0xffff000 [18]=0xffff000 [19]=0xffff000 [20]=0xffff000 [21]=0xffff000 [22]=0xffff000 [23]=0xffff000 [24]=0xffff000 [25]=0xffff000 [26]=0xffff000 [27]=0xffff000 [28]=0xffff000 [29]=0xffff000 [30]=0xffff000 [31]=0xffff000 [32]=0x20000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0xffff000 [5]=0xffff000 [6]=0xffff000 [7]=0xffff000 [8]=0xffff000 [9]=0xffff000 [10]=0xffff000 [11]=0xffff000 [12]=0xffff000 [13]=0xffff000 [14]=0xffff000 [15]=0xffff000 [16]=0xffff000 [17]=0xffff000 [18]=0xffff000 [19]=0xffff000 [20]=0xffff000 [21]=0xffff000 [22]=0xffff000 [23]=0xffff000 [24]=0xffff000 [25]=0xffff000 [26]=0xffff000 [27]=0xffff000 [28]=0xffff000 [29]=0xffff000 [30]=0xffff000 [31]=0xffff000 [32]=0x20000
Test pattern pass
host-to-card DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1771.570230 (Mbytes/sec)
  Mean = 1771.573312 (Mbytes/sec)
   Max = 1771.576395 (Mbytes/sec)
card-to-host DMA timing for 2 transfers of 8589934592 bytes:
   Min = 1291.510233 (Mbytes/sec)
  Mean = 1291.510312 (Mbytes/sec)
   Max = 1291.510392 (Mbytes/sec)
Testing NiteFury_dma_ddr3 design with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1332.377110 (Mbytes/sec)
  Mean = 1332.586621 (Mbytes/sec)
   Max = 1332.816166 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1150.302518 (Mbytes/sec)
  Mean = 1150.314626 (Mbytes/sec)
   Max = 1150.325256 (Mbytes/sec)
