1. Tests using TOSING_160T_dma_ddr3 design with no IOMMU
========================================================

The TOSING_160T_dma_ddr3 design was used in a i5-2310 system which doesn't have an IOMMU.

The following was temporarily added to the command line to allocate physical memory for the cmem_gdb_access driver:
  memmap=2050M$4G,1M$6146M

Using default arguments, which results in transfers which are not word aligned:
[mr_halfword@haswell-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f2c2a43a000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f2be9a24000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f2ba9a24000 
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 836.020419 (Mbytes/sec)
  Mean = 836.178502 (Mbytes/sec)
   Max = 836.356822 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.552247 (Mbytes/sec)
  Mean = 834.702048 (Mbytes/sec)
   Max = 834.819760 (Mbytes/sec)
Memory Driver closed 

With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@haswell-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f0b4e281000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f0b0d86b000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f0acd86b000 
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 835.930008 (Mbytes/sec)
  Mean = 836.092384 (Mbytes/sec)
   Max = 836.261624 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.649540 (Mbytes/sec)
  Mean = 834.695353 (Mbytes/sec)
   Max = 834.785671 (Mbytes/sec)
Memory Driver closed 


2. Test using Nitefury Project-0 and dma_blkram designs with IOMMU enabled
==========================================================================

The Nitefury Project-0 and dma_blkram designs were used in a W-2123 system with the IOMMU and secure boot enabled. 

Using default arguments, which for the Nitefury Project-0 design results in transfers which are not word aligned:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 87
Enabling bus master for 0000:2e:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1446.425221 (Mbytes/sec)
  Mean = 1755.580095 (Mbytes/sec)
   Max = 1757.884902 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1375.132746 (Mbytes/sec)
  Mean = 1405.979819 (Mbytes/sec)
   Max = 1406.244636 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:2e:00.0 IOMMU group 87
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.816988 (Mbytes/sec)
  Mean = 1325.819613 (Mbytes/sec)
   Max = 1325.820986 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.562019 (Mbytes/sec)
  Mean = 1130.570527 (Mbytes/sec)
   Max = 1130.578031 (Mbytes/sec)


With page size alignment for transfers, which has no effect on the transfer rate:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Enabling bus master for 0000:15:00.0
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 87
Enabling bus master for 0000:2e:00.0
Testing dma_blkram (TEF1001) design with memory size 0x120000
PCI device 0000:15:00.0 IOMMU group 41
Size of DMA descriptors used for h2c: [0]=0x120000
Size of DMA descriptors used for c2h: [0]=0x120000
Test pattern pass
host-to-card DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1706.175450 (Mbytes/sec)
  Mean = 1754.846234 (Mbytes/sec)
   Max = 1756.536863 (Mbytes/sec)
card-to-host DMA timing for 14564 transfers of 1179648 bytes:
   Min = 1361.628255 (Mbytes/sec)
  Mean = 1405.971441 (Mbytes/sec)
   Max = 1406.229548 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:2e:00.0 IOMMU group 87
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.805850 (Mbytes/sec)
  Mean = 1325.818622 (Mbytes/sec)
   Max = 1325.820957 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.579250 (Mbytes/sec)
  Mean = 1130.612202 (Mbytes/sec)
   Max = 1130.638191 (Mbytes/sec)


3. Tests using TOSING_160T_dma_ddr3 design with broken timing and with no IOMMU
===============================================================================

The tests in 1) used the TOSING_160T_dma_ddr3 design from commit 2c8c960, and the resulting bandwidth was lower
than with the Nitefury Project-0 even though:
a. TOSING_160T_dma_ddr3 should have DDR3-1066 with a 32-bit bus.
b. Nitefury Project-0 has DDR3-800 with a 16-bit bus.

I.e. lower bandwidth for TOSING_160T_dma_ddr3 even though in theory the DDR3 interface had more bandwidth.

Tried changing the PHYRatio from 4:1 to 2:1, which also involved increasing the TimePeriod from 1500 to 1875
in order allow the PHYRatio to be changed.

Differences were:
$ git diff -U0 | cat
diff --git a/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl b/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
index 655bda2..471322f 100644
--- a/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
+++ b/fpga_tests/TOSING_160T_dma_ddr3/create_project.tcl
@@ -6 +6 @@
-# Generated by Vivado on Sat Sep 09 19:56:21 BST 2023
+# Generated by Vivado on Sun Oct 01 22:54:57 BST 2023
@@ -172,0 +173 @@ set_property -name "top" -value "TOSING_160T_dma_ddr3_wrapper" -objects $obj
+set_property -name "top_auto_set" -value "0" -objects $obj
@@ -221,0 +223 @@ set_property -name "top" -value "TOSING_160T_dma_ddr3_wrapper" -objects $obj
+set_property -name "top_auto_set" -value "0" -objects $obj
@@ -318 +320 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <TimePeriod>1500</TimePeriod>}
+   puts $mig_prj_file {    <TimePeriod>1875</TimePeriod>}
@@ -320 +322 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <PHYRatio>4:1</PHYRatio>}
+   puts $mig_prj_file {    <PHYRatio>2:1</PHYRatio>}
@@ -323 +325 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <MMCM_VCO>666</MMCM_VCO>}
+   puts $mig_prj_file {    <MMCM_VCO>1066</MMCM_VCO>}
@@ -425 +427 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <mrCasLatency name="CAS Latency">9</mrCasLatency>}
+   puts $mig_prj_file {    <mrCasLatency name="CAS Latency">7</mrCasLatency>}
@@ -439 +441 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-   puts $mig_prj_file {    <mr2CasWriteLatency name="CAS write latency">7</mr2CasWriteLatency>}
+   puts $mig_prj_file {    <mr2CasWriteLatency name="CAS write latency">6</mr2CasWriteLatency>}
@@ -531 +533 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-  set str_mig_file_name mig_b.prj
+  set str_mig_file_name mig_a.prj
@@ -539 +541 @@ proc write_mig_file_TOSING_160T_dma_ddr3_mig_7series_0_0 { str_mig_prj_filepath
-    CONFIG.XML_INPUT_FILE {mig_b.prj} \
+    CONFIG.XML_INPUT_FILE {mig_a.prj} \

However, with this change then Vivado reported 806 failing timing endpoints for Intra-Clock Paths for clk_pll_i
Worst case slack was -1.8 ns for setup.

Trying this FPGA image did report data validation errors, and didn't change the bandwidth achieved by the DMA transfers:
$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -a 4096
Opening device 0000:01:00.0 (10ee:7024) with IOMMU group 0
Enabling bus master for 0000:01:00.0
Testing TOSING_160T_dma_ddr3 design with memory size 0x40000000
PCI device 0000:01:00.0 IOMMU group 0
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7f64ec05c000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7f64ab646000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7f646b646000 
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
DDR word[274] actual=0xaa541f0a expected=0xaa541f02
DDR word[1586] actual=0x476aedaa expected=0x476aeda2
DDR word[594] actual=0x20bf74a expected=0x20bf742
DDR word[2381] actual=0x2b742e8f expected=0x2b74224b
DDR word[1186] actual=0x47d5195a expected=0x47d51952
DDR word[461] actual=0x3e93a70f expected=0x3e93a2cb
DDR word[460] actual=0x31081e1c expected=0x39081e1c
DDR word[209] actual=0xb57114af expected=0xb571142f
DDR word[2125] actual=0x85279b8f expected=0x8527114b
DDR word[562] actual=0xa7b039aa expected=0xa7b039a2
DDR word[50] actual=0xa48adfaa expected=0xa48adfa2
DDR word[737] actual=0xc1f752bf expected=0xc1f7523f
DDR word[2545] actual=0x31ae79cf expected=0x31ae794f
DDR word[844] actual=0x1ce5bd9c expected=0x18e5bd9c
DDR word[205] actual=0x1664140f expected=0x166491cb
DDR word[322] actual=0x4b23f17a expected=0x4b23f172
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 834.598262 (Mbytes/sec)
  Mean = 835.675384 (Mbytes/sec)
   Max = 836.468913 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 833.912205 (Mbytes/sec)
  Mean = 834.458438 (Mbytes/sec)
   Max = 834.801162 (Mbytes/sec)
Memory Driver closed 

Therefore, didn't commit the change to the DDR3 MIG configuration in the TOSING_160T_dma_ddr3 design.

However, this did show the use of the test pattern in test_dma_accessible_memory detecting errors.

As for the achieved DMA bandwidth with TOSING_160T_dma_ddr3 being lower than Nitefury Project-0,
the different FPGA images were tested in different PC and so the PCIe Root Bridge / memory in the
two PCs might be impacting the achieved bandwidth.


4. Test using Nitefury Project-0 and TEF1001_dma_ddr3 designs with IOMMU enabled; issues with host memory sizes
===============================================================================================================

The Nitefury Project-0 and TEF1001_dma_ddr3 designs were used in a W-2123 system with the IOMMU and secure boot enabled. 

This did show some issues with host memory sizes.

The TEF1001_dma_ddr3 has 8G of DDR3 memory, and the PC only has 16G of memory.
Therefore, with default command line arguments in which a host buffer of the entire DDR3 memory is attempted
to be allocated for both host-to-card and card-to-host transfers expected the test for TEF1001_dma_ddr3
to fail to allocate memory, although both 8G allocations failed:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
VFIO_IOMMU_MAP_DMA of size 8589934592 failed : Operation not permitted
VFIO_IOMMU_MAP_DMA of size 8589934592 failed : Operation not permitted
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.814544 (Mbytes/sec)
  Mean = 1325.823781 (Mbytes/sec)
   Max = 1325.825198 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.581147 (Mbytes/sec)
  Mean = 1130.601240 (Mbytes/sec)
   Max = 1130.625491 (Mbytes/sec)


In use a 1G size for host-to-card and card-to-host transfers the TEF1001_dma_ddr3 test passes,
but the Nitefury Project-0 then fails during VFIO_IOMMU_MAP_DMA on attempting to allocate the 2nd host buffer:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -l 1073741824 -m 1073741824
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1771.180385 (Mbytes/sec)
  Mean = 1771.198526 (Mbytes/sec)
   Max = 1771.204185 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1291.505285 (Mbytes/sec)
  Mean = 1291.506313 (Mbytes/sec)
   Max = 1291.507205 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
VFIO_IOMMU_MAP_DMA of size 1073741824 failed : Operation not permitted


With a 512M size for host-to-card transfers and a 1G size for card-to-host transfers the tests on both FPGAs pass:
[mr_halfword@skylake-alma release]$ xilinx_dma_bridge_for_pcie/test_dma_accessible_memory -l 1073741824 -m 536870912
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 25
Enabling bus master for 0000:15:00.0
Opening device 0000:30:00.0 (10ee:7011) with IOMMU group 89
Enabling bus master for 0000:30:00.0
Testing TEF1001_dma_ddr3 design with memory size 0x200000000
PCI device 0000:15:00.0 IOMMU group 25
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0x2
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 32 transfers of 536870912 bytes:
   Min = 1771.146331 (Mbytes/sec)
  Mean = 1771.212862 (Mbytes/sec)
   Max = 1771.252126 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1291.505793 (Mbytes/sec)
  Mean = 1291.507043 (Mbytes/sec)
   Max = 1291.507871 (Mbytes/sec)
Testing Nitefury Project-0 design version 0x2 with memory size 0x40000000
PCI device 0000:30:00.0 IOMMU group 89
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0x2
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
host-to-card DMA timing for 32 transfers of 536870912 bytes:
   Min = 1325.773629 (Mbytes/sec)
  Mean = 1325.811476 (Mbytes/sec)
   Max = 1325.816964 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.579775 (Mbytes/sec)
  Mean = 1130.596196 (Mbytes/sec)
   Max = 1130.610757 (Mbytes/sec)
