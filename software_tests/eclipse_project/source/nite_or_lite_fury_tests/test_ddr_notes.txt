1. Alignment for descriptor size doesn't have any measurable effect on Host To Card transfer speed
==================================================================================================

The default is that the sizes of all but the final descriptors for the final h2c descriptor were an odd number
of bytes which could have led to inefficient transfers:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Print wrote 0x40000000 bytes to card using DMA in 809873136 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869331 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870773 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869038 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868758 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868589 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809878551 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869286 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872494 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809875564 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868619 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869684 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869790 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868585 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868708 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869213 ns
Test pattern pass


Running with a command line option to specify page size alignment didn't have any effect on the transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809870948 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869768 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869140 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869878 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869697 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868489 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869273 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869195 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869766 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868857 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869318 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869569 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870026 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868972 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868897 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868942 ns
Test pattern pass


2. Can use a POSIX shared memory file, rather than heap, for the buffers used for VFIO DMA
==========================================================================================

The default is that the heap was used to allocate buffers used for VFIO DMA, where the heap is a private mapping
in the process.

Added an option to allow POSIX shared memory files to be used instead of the heap.

The test ran successfully when using the POSIX shared memory files, with no measurable effect on Host To Card transfer speed
compared to when the VFIO DMA buffers were allocated from the heap:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -b shared_memory
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Print wrote 0x40000000 bytes to card using DMA in 809871617 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869833 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869974 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870190 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869799 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870513 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873538 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869619 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870502 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871523 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870283 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869332 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873048 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868583 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869244 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809875086 ns
Test pattern pass


While the above test is running the POSIX shared memory files were:
[mr_halfword@skylake-alma ~]$ ls -l /dev/shm/
total 1048584
-rwxrwxr-x. 1 mr_halfword mr_halfword       4096 Feb 12 11:04 vfio_buffer_pid-6570_iova-0
-rwxrwxr-x. 1 mr_halfword mr_halfword       4096 Feb 12 11:04 vfio_buffer_pid-6570_iova-1073745920
-rwxrwxr-x. 1 mr_halfword mr_halfword 1073741824 Feb 12 11:04 vfio_buffer_pid-6570_iova-4096

For this initial test, the POSIX shared memory files were only used for VFIO DMA in the process which created them.
The initial thought was that using POSIX shared memory files for VFIO DMA buffers could provide a mean for multiple
processes to be able to perform VFIO DMA for one device to avoid the limitation that only one process can open
the IOMMU group file. However, the limitations are:
a. Doesn't allow the PCI BARs to be mapped by VFIO into multiple processes.
b. Means the "primary" process which creates the POSIX shared memory files would have the buffers mapped into
   it's virtual address space even though wasn't actually performing any VFIO DMA.

Instead try opening the group and container file descriptors in a "primary" process and use SCM_RIGHTS on
a UNIX domain socket to send the file descriptors to "secondary" process(es).
From a quick look DPDK appears to do this (haven't traced all the code).


3. VFIO DMA worked for the Xilinx "DMA/Bridge Subsystem for PCI Express" even when wasn't enabled as BusMaster
==============================================================================================================

Noticed that test_ddr was able to successfully perform DMA, while dump_info_pciutils was always reporting BusMaster-
for the PCI device.

Added code into initialise_x2x_transfer_context() to set the PCI_COMMAND_MASTER bit if clear.
Every time test_ddr runs it has to set the PCI_COMMAND_MASTER bit and:
a. dump_info_pciutils reports BusMaster+ while test_ddr is running.
b. Once test_ddr completes dump_info_pciutils reports BusMaster-
   Looks like VFIO clears the BusMaster flag when the device is closed.

Not sure why the Xilinx "DMA/Bridge Subsystem for PCI Express" was able to perform DMA even when BusMaster was clear.
E.g. does this mean the FPGA isn't meeting the PCIe spec?

Enabling BusMaster hasn't changed the Host to Card transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
    control: I/O- Mem+ BusMaster-
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809871747 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871007 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868700 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868640 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869687 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868707 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868556 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872446 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868450 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869681 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869359 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868671 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869120 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868681 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868837 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870995 ns
Test pattern pass


4. Using 2M huge pages doesn't have any measurable effect on Host To Card transfer speed
========================================================================================

Added an option to allocate the VFIO DMA buffers from huge pages.

Huge pages allocated using:
$ sudo dpdk-hugepages.py --setup 2048M

Using 2M huge pages doesn't have any measurable effect on Host To Card transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096 -b huge_pages
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
    control: I/O- Mem+ BusMaster-
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809869912 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869526 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809867983 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868116 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868156 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809874158 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872146 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869851 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809867926 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868759 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871451 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869659 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873764 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868904 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868880 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868698 ns
Test pattern pass
munmap(4096) failed : Invalid argument
munmap(4096) failed : Invalid argument
