1. Alignment for descriptor size doesn't have any measurable effect on Host To Card transfer speed
==================================================================================================

The default is that the sizes of all but the final descriptors for the final h2c descriptor were an odd number
of bytes which could have led to inefficient transfers:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Print wrote 0x40000000 bytes to card using DMA in 809873136 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869331 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870773 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869038 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868758 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868589 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809878551 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869286 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872494 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809875564 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868619 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869684 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869790 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868585 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868708 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869213 ns
Test pattern pass


Running with a command line option to specify page size alignment didn't have any effect on the transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809870948 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869768 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869140 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869878 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869697 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868489 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869273 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869195 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869766 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868857 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869318 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869569 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870026 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868972 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868897 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868942 ns
Test pattern pass


2. Can use a POSIX shared memory file, rather than heap, for the buffers used for VFIO DMA
==========================================================================================

The default is that the heap was used to allocate buffers used for VFIO DMA, where the heap is a private mapping
in the process.

Added an option to allow POSIX shared memory files to be used instead of the heap.

The test ran successfully when using the POSIX shared memory files, with no measurable effect on Host To Card transfer speed
compared to when the VFIO DMA buffers were allocated from the heap:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -b shared_memory
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Print wrote 0x40000000 bytes to card using DMA in 809871617 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869833 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869974 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870190 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869799 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870513 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873538 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869619 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870502 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871523 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870283 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869332 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873048 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868583 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869244 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809875086 ns
Test pattern pass


While the above test is running the POSIX shared memory files were:
[mr_halfword@skylake-alma ~]$ ls -l /dev/shm/
total 1048584
-rwxrwxr-x. 1 mr_halfword mr_halfword       4096 Feb 12 11:04 vfio_buffer_pid-6570_iova-0
-rwxrwxr-x. 1 mr_halfword mr_halfword       4096 Feb 12 11:04 vfio_buffer_pid-6570_iova-1073745920
-rwxrwxr-x. 1 mr_halfword mr_halfword 1073741824 Feb 12 11:04 vfio_buffer_pid-6570_iova-4096

For this initial test, the POSIX shared memory files were only used for VFIO DMA in the process which created them.
The initial thought was that using POSIX shared memory files for VFIO DMA buffers could provide a mean for multiple
processes to be able to perform VFIO DMA for one device to avoid the limitation that only one process can open
the IOMMU group file. However, the limitations are:
a. Doesn't allow the PCI BARs to be mapped by VFIO into multiple processes.
b. Means the "primary" process which creates the POSIX shared memory files would have the buffers mapped into
   it's virtual address space even though wasn't actually performing any VFIO DMA.

Instead try opening the group and container file descriptors in a "primary" process and use SCM_RIGHTS on
a UNIX domain socket to send the file descriptors to "secondary" process(es).
From a quick look DPDK appears to do this (haven't traced all the code).


3. VFIO DMA worked for the Xilinx "DMA/Bridge Subsystem for PCI Express" even when wasn't enabled as BusMaster
==============================================================================================================

Noticed that test_ddr was able to successfully perform DMA, while dump_info_pciutils was always reporting BusMaster-
for the PCI device.

Added code into initialise_x2x_transfer_context() to set the PCI_COMMAND_MASTER bit if clear.
Every time test_ddr runs it has to set the PCI_COMMAND_MASTER bit and:
a. dump_info_pciutils reports BusMaster+ while test_ddr is running.
b. Once test_ddr completes dump_info_pciutils reports BusMaster-
   Looks like VFIO clears the BusMaster flag when the device is closed.

Not sure why the Xilinx "DMA/Bridge Subsystem for PCI Express" was able to perform DMA even when BusMaster was clear.
E.g. does this mean the FPGA isn't meeting the PCIe spec?

Enabling BusMaster hasn't changed the Host to Card transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
    control: I/O- Mem+ BusMaster-
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809871747 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871007 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868700 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868640 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869687 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868707 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868556 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872446 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868450 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869681 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869359 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868671 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869120 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868681 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868837 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809870995 ns
Test pattern pass


4. Using 2M huge pages doesn't have any measurable effect on Host To Card transfer speed
========================================================================================

Added an option to allocate the VFIO DMA buffers from huge pages.

Huge pages allocated using:
$ sudo dpdk-hugepages.py --setup 2048M

Using 2M huge pages doesn't have any measurable effect on Host To Card transfer speed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr -a 4096 -b huge_pages
Opening device 0000:15:00.0 (10ee:7024) with IOMMU group 41
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
    control: I/O- Mem+ BusMaster-
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Print wrote 0x40000000 bytes to card using DMA in 809869912 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869526 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809867983 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868116 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868156 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809874158 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809872146 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869851 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809867926 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868759 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809871451 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809869659 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809873764 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868904 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868880 ns
Test pattern pass
Print wrote 0x40000000 bytes to card using DMA in 809868698 ns
Test pattern pass
munmap(4096) failed : Invalid argument
munmap(4096) failed : Invalid argument


5. DMA transfer timing when using physical addresses and no IOMMU
=================================================================

This is a test where the DMA engine in the FPGA is given physical addresses in the host memory, and the IOMMU is not used.

To configure the PC for the test:
a. Secure boot disabled in the BIOS
b. During boot use grub to temporarily edit the Linux command line to:
   - Remove the intel_iommu=on option, so Linux doesn't enable the IOMMU
   - Add 2050M$4G,1M$6146M to the memmap options to reserve physical addresses to be used for DMA.

The complete command line:
$ cat /proc/cmdline 
BOOT_IMAGE=(hd1,gpt2)/vmlinuz-4.18.0-425.13.1.el8_7.x86_64 root=/dev/mapper/almalinux_skylake--alma-root ro resume=/dev/mapper/almalinux_skylake--alma-swap rd.lvm.lv=almalinux_skylake-alma/root rd.lvm.lv=almalinux_skylake-alma/swap rhgb quiet memmap=10444K%0x6c144000-4+2,2050M$4G,1M$6146M

After booted loaded the module for the physical memory allocator:
[mr_halfword@skylake-alma ~]$ cd ~/cmem_gdb_access/module/
[mr_halfword@skylake-alma module]$ sudo ./load.sh 
[sudo] password for mr_halfword: 

Bound the vfio-pci driver to the FPGA devices, where the script determined it needed to enable noiommu mode:
[mr_halfword@skylake-alma ~]$ cd ~/fpga_sio/software_tests/eclipse_project/
[mr_halfword@skylake-alma eclipse_project]$ ./bind_xilinx_devices_to_vfio.sh 
No IOMMUs present
Loading vfio module and enabling NOIOMMU (this taints the Kernel)
[sudo] password for mr_halfword: 
Loading vfio-pci module
vfio-pci
0000:15:00.0
Bound vfio-pci driver to 0000:15:00.0 10ee:7024 [0002:0001]
Giving user permission to IOMMU group noiommu-0 for 0000:15:00.0 10ee:7024 [0002:0001]
vfio-pci
0000:2e:00.0
Bound vfio-pci driver to 0000:2e:00.0 10ee:7011 [0000:0000]
Giving user permission to IOMMU group noiommu-1 for 0000:2e:00.0 10ee:7011 [0000:0000]

First attempt to run the test program as a normal user failed, since hadn't given the cap_sys_rawio capability which is
required to allow a normal user to open a device in noiommu mode:
$ bin/release/nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 1
  No permission to open /dev/vfio/noiommu-1. Try:
sudo setcap cap_sys_rawio=ep /home/mr_halfword/fpga_sio/software_tests/eclipse_project/bin/release/nite_or_lite_fury_tests/test_ddr

Gave permission as suggested:
$ sudo setcap cap_sys_rawio=ep /home/mr_halfword/fpga_sio/software_tests/eclipse_project/bin/release/nite_or_lite_fury_tests/test_ddr

Ran the test successfully, with the transfers given page alignment:
$ bin/release/nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 1
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 1
Opened DMA MEM device : /dev/cmem (dev_desc = 0x00000007)
Debug: mmap param length 0x1000, Addr: 0x100000000 
Buff num 0: Phys addr : 0x100000000 User Addr: 0x7fc5d8a4f000 
Debug: mmap param length 0x40000000, Addr: 0x100001000 
Buff num 0: Phys addr : 0x100001000 User Addr: 0x7fc598037000 
Debug: mmap param length 0x40000000, Addr: 0x140001000 
Buff num 0: Phys addr : 0x140001000 User Addr: 0x7fc558037000 
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.816633 (Mbytes/sec)
  Mean = 1325.820264 (Mbytes/sec)
   Max = 1325.821736 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.563646 (Mbytes/sec)
  Mean = 1130.585825 (Mbytes/sec)
   Max = 1130.612132 (Mbytes/sec)
Memory Driver closed 

After performing the test their were multiple reasons why the Kernel had been tained:
$ ~/Downloads/kernel-chktaint 
Kernel is "tainted" for the following reasons:
 * taint requested by userspace application (#6)
 * externally-built ('out-of-tree') module was loaded  (#12)
 * unsigned module was loaded (#13)
For a more detailed explanation of the various taint flags see
 Documentation/admin-guide/tainted-kernels.rst in the Linux kernel sources
 or https://kernel.org/doc/html/latest/admin-guide/tainted-kernels.html
Raw taint value as int/string: 12352/'G     U     OE     '

The "taint requested by userspace application (#6)" is due to opening a device in noiommu mode:
$ dmesg|grep taint
[   99.065196] cmem_dev: loading out-of-tree module taints kernel.
[   99.065269] cmem_dev: module verification failed: signature and/or required key missing - tainting kernel
[  115.724514] vfio-pci 0000:15:00.0: Adding kernel taint for vfio-noiommu group on device
[  115.783225] vfio-pci 0000:2e:00.0: Adding kernel taint for vfio-noiommu group on device


6. DMA transfer timing when using the IOMMU
===========================================

This is a test where the IOMMU is use, and the DMA engine in the FPGA is given iova "virtual" IO addresses.

To configure the PC for the test:
a. Enabled secure boot in the BIOS
b. Left the command line at the defaults, which enables the IOMMU and doesn't reserve any physical memory

The complete command line:
$ cat /proc/cmdline 
BOOT_IMAGE=(hd1,gpt2)/vmlinuz-4.18.0-425.13.1.el8_7.x86_64 root=/dev/mapper/almalinux_skylake--alma-root ro resume=/dev/mapper/almalinux_skylake--alma-swap rd.lvm.lv=almalinux_skylake-alma/root rd.lvm.lv=almalinux_skylake-alma/swap rhgb quiet memmap=10444K%0x6c144000-4+2 intel_iommu=on

dmesg output showing Kernel lockdown is active due to secure boot:
$ dmesg|grep lockdown
[    0.000000] Kernel is locked down from EFI secure boot; see man kernel_lockdown.7
[    9.882160] Lockdown: swapper/0: Hibernation is restricted; see man kernel_lockdown.7
[   32.995344] Lockdown: x86_energy_perf: Direct MSR access is restricted; see man kernel_lockdown.7

The test_ddr program had been re-built following the previous noiommu test, so didn't have any capabilities set.

Bound the vfio-pci driver to the FPGA devices, which determined IOMMU devices were present:
[mr_halfword@skylake-alma ~]$ cd ~/fpga_sio/software_tests/eclipse_project/
[mr_halfword@skylake-alma eclipse_project]$ ./bind_xilinx_devices_to_vfio.sh 
IOMMU devices present: dmar0  dmar1  dmar2  dmar3
Loading vfio-pci module
[sudo] password for mr_halfword: 
vfio-pci
0000:15:00.0
Bound vfio-pci driver to 0000:15:00.0 10ee:7024 [0002:0001]
Giving user permission to IOMMU group 41 for 0000:15:00.0 10ee:7024 [0002:0001]
vfio-pci
0000:2e:00.0
Bound vfio-pci driver to 0000:2e:00.0 10ee:7011 [0000:0000]
Giving user permission to IOMMU group 86 for 0000:2e:00.0 10ee:7011 [0000:0000]

Successfull run the test:
$ bin/release/nite_or_lite_fury_tests/test_ddr -a 4096
Opening device 0000:2e:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2e:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2e:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Size of DMA descriptors used for c2h: [0]=0xffff000 [1]=0xffff000 [2]=0xffff000 [3]=0xffff000 [4]=0x4000
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.803357 (Mbytes/sec)
  Mean = 1325.815926 (Mbytes/sec)
   Max = 1325.818270 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.552347 (Mbytes/sec)
  Mean = 1130.584317 (Mbytes/sec)
   Max = 1130.599719 (Mbytes/sec)

After the test the Kernel isn't tainted:
$ ~/Downloads/kernel-chktaint 
Kernel not Tainted

Comparing the DMA transfer rate with and without the IOMMU enabled shows no difference.


7. Trying different FPGA images for NiteFury
============================================

For these tests the IOMMU and secure boot were enabled.

7.1. Test with FPGA booting from flash as delivered
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2f:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.808695 (Mbytes/sec)
  Mean = 1325.821472 (Mbytes/sec)
   Max = 1325.825231 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.589983 (Mbytes/sec)
  Mean = 1130.629860 (Mbytes/sec)
   Max = 1130.652823 (Mbytes/sec)
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_general 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
Found NiteFury board version 0x2 for PCI device 0000:2f:00.0 IOMMU group 86
Temp C=41.7
VCCInt=1.03
vccaux=1.78
vbram=1.03


7.2. Test FPGA programmed from JTAG on a re-build
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just built https://github.com/RHSResearchLLC/NiteFury-and-LiteFury unsing Vivado 2022.1 under Windows 10.
Then programmed the FPGA over JTAG from the locally built bitstream, and rebooted the PC to cause to re-enumerate.

The tests still passed:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2f:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.817668 (Mbytes/sec)
  Mean = 1325.820795 (Mbytes/sec)
   Max = 1325.824082 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.502040 (Mbytes/sec)
  Mean = 1130.526018 (Mbytes/sec)
   Max = 1130.540038 (Mbytes/sec)
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_general
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
Found NiteFury board version 0x2 for PCI device 0000:2f:00.0 IOMMU group 86
Temp C=42.3
VCCInt=1.03
vccaux=1.78
vbram=1.03


7.3. FPGA reprogrammed with incorrect constraint processing causing PCIe to enumerate as only one lane
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As part of moving the FPGA build to use a TCL project creation script, had changed how the constraint files are used,
but had the side effect that the "early" property wasn't set on the early.xdc.

That had the effect of changing the order of PCIe pins used such that the PCIe only enumerated using 1 lane, rather than 4:
$ sudo lspci -nn -vvv -d 10ee:7011 | grep Width
        LnkCap: Port #0, Speed 5GT/s, Width x4, ASPM L0s, Exit Latency L0s unlimited
        LnkSta: Speed 5GT/s (ok), Width x1 (downgraded)

Which resulted in test_ddr reporting a lower transfer speed, approx 0.34 of that with a 4 lane width:
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2f:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 449.573667 (Mbytes/sec)
  Mean = 449.656065 (Mbytes/sec)
   Max = 449.721081 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 450.774878 (Mbytes/sec)
  Mean = 450.775340 (Mbytes/sec)
   Max = 450.775652 (Mbytes/sec)
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_general 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
Found NiteFury board version 0x2 for PCI device 0000:2f:00.0 IOMMU group 86
Temp C=44.0
VCCInt=1.03
vccaux=1.78
vbram=1.03


7.4. FPGA reprogrammed from JTAG following use of project creation script under Windows 10
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The FPGA was built using the create_project.tcl script added in
https://github.com/Chester-Gillon/NiteFury-and-LiteFury/commit/348fdf999c453ccf485513ab7ee2b9a05043feac
using Vivado 2022.1 under Windows 10.

[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2f:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.808155 (Mbytes/sec)
  Mean = 1325.820799 (Mbytes/sec)
   Max = 1325.823659 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.516961 (Mbytes/sec)
  Mean = 1130.530155 (Mbytes/sec)
   Max = 1130.540228 (Mbytes/sec)
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_general 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
Found NiteFury board version 0x2 for PCI device 0000:2f:00.0 IOMMU group 86
Temp C=45.8
VCCInt=1.03
vccaux=1.78
vbram=1.03


7.5. FPGA reprogrammed from JTAG following use of project creation script under Ubuntu 18.04
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The FPGA was built using the commit in
https://github.com/Chester-Gillon/NiteFury-and-LiteFury/commit/ed20c02827ab61115a7aac8f42805a5329da417a
using Vivado 2022.1 under Ubuntu 18.04. Which was a work-around to allow the correct PCIe pin placement when
building under Linux.

The results show the FPGA has enumerated at the expected speed and width, and the test_ddr throughput matches
those from when Windows was used for the build:
$ sudo lspci -nn -vvv -d 10ee:7011 | grep Width
        LnkCap: Port #0, Speed 5GT/s, Width x4, ASPM L0s, Exit Latency L0s unlimited
        LnkSta: Speed 5GT/s (ok), Width x4 (ok)

[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_ddr 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
    control: I/O- Mem+ BusMaster+
Testing NiteFury board version 0x2 with DDR size 0x40000000 for PCI device 0000:2f:00.0 IOMMU group 86
Size of DMA descriptors used for h2c: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Size of DMA descriptors used for c2h: [0]=0xfffffff [1]=0xfffffff [2]=0xfffffff [3]=0xfffffff [4]=0x4
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
Test pattern pass
host-to-card DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1325.821016 (Mbytes/sec)
  Mean = 1325.823761 (Mbytes/sec)
   Max = 1325.824758 (Mbytes/sec)
card-to-host DMA timing for 16 transfers of 1073741824 bytes:
   Min = 1130.584007 (Mbytes/sec)
  Mean = 1130.599049 (Mbytes/sec)
   Max = 1130.626712 (Mbytes/sec)
[mr_halfword@skylake-alma release]$ nite_or_lite_fury_tests/test_general 
Opening device 0000:2f:00.0 (10ee:7011) with IOMMU group 86
Enabling bus master for 0000:2f:00.0
Found NiteFury board version 0x2 for PCI device 0000:2f:00.0 IOMMU group 86
Temp C=46.9
VCCInt=1.03
vccaux=1.78
vbram=1.03
